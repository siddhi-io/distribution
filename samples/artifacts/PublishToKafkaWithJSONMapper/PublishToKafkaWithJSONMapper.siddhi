@App:name("PublishToKafkaWithJSONMapper")
@App:description("Receives events via simulation and publish them to a kafka topic in json format")

/*

Purpose:
    This application demonstrates how to use the Kafka transport and JSONMapper to publish events. Events which are sent from the simulation are consumed and written to a kafka topic in json format.

Prerequisites:
    1) The following steps must be executed to enable Siddhi to publish events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding.
        a) Download the Kafka broker from here: https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz
        b) Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {Siddhi_Distribution_Home}/libs directory as follows.
              i) Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory.
                 * kafka_2.11-0.10.2.1.jar
                 * kafka-clients-0.10.2.1.jar
                 * metrics-core-2.2.0.jar
                 * scala-library-2.11.8.jar
                 * scala-parser-combinators_2.11-1.0.4.jar
                 * zkclient-0.10.jar
                 * zookeeper-3.4.9.jar
             ii) Create another directory named {Destination} in a preferred location in your machine.
            iii) To convert all the Kafka jars you copied into the {Source} directory, issue the following command,
                 * For Windows: {Siddhi_Distribution_Home}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path>
                 * For Linux: sh {Siddhi_Distribution_Home}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path>
             iv) Add the OSGI converted kafka libs from {Destination} directory to {Siddhi_Distribution_Home}/lib
              v) Add the original Kafka libs from {Source} to {Siddhi_Distribution_Home}/samples/sample-clients/lib
             vi) Navigate to {KafkaHome} and start zookeeper node using "sh bin/zookeeper-server-start.sh config/zookeeper.properties" command
            vii) Navigate to {KafkaHome} and start Kafka server node using "sh bin/kafka-server-start.sh config/server.properties" command
           viii) Start the server: sh editor.sh
    2) Save this sample

Executing the Sample:
    1) Start the Siddhi application by clicking on 'Run'
    2) If the Siddhi application starts successfully, the following messages would be shown on the console,
        * PublishToKafkaWithJSONMapper.siddhi - Started Successfully!

Testing the Sample:
    1) Navigate to {Siddhi_Distribution_Home}/samples/sample-clients/kafka-consumer and run the following command
        ant -DtopicList=kafka_json_topic  -DpartitionList=0 -Dtype=json
    2) Simulate single events:
        a) Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation'
        b) Select 'PublishToKafkaWithJSONMapper' as 'Siddhi App Name' and select 'SweetProductionStream' as 'Stream Name
        c) Provide attribute values, and then click Send.

Viewing the Results:
    See the output events received by Sink Kafka Topic (named 'kafka_json_topic') being logged on the 'kafka-consumer' console.
*/

define stream SweetProductionStream (name string, amount double);

@sink(type='kafka',
      topic='kafka_json_topic',
      partition.no='0',
      bootstrap.servers='localhost:9092',
      @map(type='json'))
define stream LowProductionAlertStream (name string, amount double);

--Send events to kafka_json_topic
@info(name='passThroughQuery')
from SweetProductionStream
select *
insert into LowProductionAlertStream;

